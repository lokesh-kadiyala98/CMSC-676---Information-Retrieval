{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d733b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "90651e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_files_path = 'input/tokens'\n",
    "\n",
    "stopwords_file_path = 'input/stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "642ca464",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "78208dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005446910858154297\n"
     ]
    }
   ],
   "source": [
    "stopwords = set()\n",
    "\n",
    "with open(stopwords_file_path, 'r') as f:\n",
    "    for word in f:\n",
    "        stopwords.add(word.strip())\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "54c30111",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq = defaultdict(int)\n",
    "words_in_doc = defaultdict(list)\n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    "tokenized_files = os.listdir(tokenized_files_path)\n",
    "\n",
    "for file in tokenized_files:\n",
    "    [doc_number, format] = file.split('.')\n",
    "    doc_number = int(doc_number)\n",
    "    file_path = 'input/tokens/' + file\n",
    "    with open(file_path) as f:\n",
    "        for word in f:\n",
    "            word = word.strip()\n",
    "            if word not in stopwords and word != '':\n",
    "                term_freq[word] += 1\n",
    "                words_in_doc[doc_number].append(word)\n",
    "                inverted_index[word].add(doc_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9b86f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq = {i: term_freq[i] for i in term_freq.keys() if term_freq[i] > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e7d09d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7576990127563477\n"
     ]
    }
   ],
   "source": [
    "for key, value in words_in_doc.items():\n",
    "    words_in_doc[key] = [i for i in value if i in term_freq.keys()]\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bd83db22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 501/501 [00:11<00:00, 43.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.17963695526123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TF = defaultdict(dict)\n",
    "IDF = {}\n",
    "\n",
    "for doc_number, words in tqdm(words_in_doc.items()):\n",
    "    for word in words:\n",
    "        if word not in TF[doc_number]:\n",
    "            TF[doc_number][word] = words.count(word) / len(words)\n",
    "        if word not in IDF:\n",
    "            IDF[word] = math.log(len(tokenized_files) / len(inverted_index[word]))\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9de2b800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.025120973587036\n"
     ]
    }
   ],
   "source": [
    "all_words = set().union(*words_in_doc.values())\n",
    "matrix = np.zeros((len(tokenized_files), len(all_words)))\n",
    "\n",
    "for doc_number, words in words_in_doc.items():\n",
    "    words_set = set(words)\n",
    "    for j, term in enumerate(all_words):\n",
    "        if term in words_set:\n",
    "            matrix[doc_number - 1, j] = TF[doc_number][term] * IDF[term]\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "706329df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "def cosine_similarity(x, y):\n",
    "    prod = np.dot(x, y)\n",
    "    x_val = np.sqrt(np.sum(x**2)) \n",
    "    y_val = np.sqrt(np.sum(y**2))\n",
    "    cosine_similarity = prod / (x_val * y_val)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "294915ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x3/lxxrgs757cq7rxgtscwjyclh0000gn/T/ipykernel_74298/974850055.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cosine_similarity = prod / (x_val * y_val)\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(tokenized_files), len(tokenized_files)))\n",
    "\n",
    "for i in range(len(tokenized_files)):\n",
    "    for j in range(i+1, len(tokenized_files)):\n",
    "        similarity_matrix[i][j] = cosine_similarity(matrix[i], matrix[j])\n",
    "np.fill_diagonal(similarity_matrix, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a41cd470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_pair(matrix):\n",
    "    matrix_copy = matrix.copy()\n",
    "    np.fill_diagonal(matrix_copy, -np.inf)\n",
    "\n",
    "    max_index_flat = np.nanargmax(matrix_copy)\n",
    "    max_indices = np.unravel_index(max_index_flat, matrix.shape)\n",
    "\n",
    "    max_similarity = matrix[max_indices]\n",
    "    document_pair = tuple(f\"{index + 1}.html\" for index in max_indices)\n",
    "    return document_pair, max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c5c46b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dissimilar_pair(matrix):\n",
    "    matrix_copy = matrix.copy()\n",
    "    np.fill_diagonal(matrix_copy, np.inf)\n",
    "\n",
    "    min_index_flat = np.nanargmin(matrix_copy)\n",
    "    min_indices = np.unravel_index(min_index_flat, matrix.shape)\n",
    "\n",
    "    min_similarity = matrix[min_indices]\n",
    "    document_pair = tuple(f\"{index + 1}.html\" for index in min_indices)\n",
    "    return document_pair, min_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "971d30b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar pair is: ('102.html', '130.html') with similarity score of 1.00000\n"
     ]
    }
   ],
   "source": [
    "highest = get_most_similar_pair(similarity_matrix)\n",
    "print(f\"The most similar pair is: {highest[0]} with similarity score of {highest[1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6bb3c3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least similar pair is:('1.html', '13.html') with similarity score of 0.0\n"
     ]
    }
   ],
   "source": [
    "lowest = get_most_dissimilar_pair(similarity_matrix)\n",
    "print(f\"The least similar pair is: {lowest[0]} with similarity score of\", lowest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bd098001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest document to the centroid of the corpus: 1.html\n"
     ]
    }
   ],
   "source": [
    "np.fill_diagonal(similarity_matrix, 1)\n",
    "corpus_centroid = np.mean(similarity_matrix, axis=1)\n",
    "\n",
    "# Find the index of the document with the highest average similarity value\n",
    "closest_document_index = np.argmax(corpus_centroid)\n",
    "print(\"Closest document to the centroid of the corpus:\",  str(closest_document_index + 1) + '.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "42f8a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agglomerative_clustering_custom(distance_matrix, method='complete', threshold=0.6):\n",
    "    # Define functions for each linkage method\n",
    "    def single_linkage(cluster1, cluster2):\n",
    "        return np.min(distance_matrix[np.ix_(cluster1, cluster2)])\n",
    "\n",
    "    def complete_linkage(cluster1, cluster2):\n",
    "        return np.max(distance_matrix[np.ix_(cluster1, cluster2)])\n",
    "\n",
    "    def centroid_linkage(cluster1, cluster2):\n",
    "        centroid1 = np.mean(distance_matrix[cluster1, :], axis=0)\n",
    "        centroid2 = np.mean(distance_matrix[cluster2, :], axis=0)\n",
    "        return np.linalg.norm(centroid1 - centroid2)\n",
    "    \n",
    "    # Mapping of method names to functions\n",
    "    methods = {\n",
    "        'single': single_linkage,\n",
    "        'complete': complete_linkage,\n",
    "        'centroid': centroid_linkage\n",
    "    }\n",
    "    \n",
    "    # Initialize clusters\n",
    "    clusters = [[i] for i in range(len(distance_matrix))]\n",
    "\n",
    "    # Initialize cluster merge function\n",
    "    cluster_dist = methods[method]\n",
    "\n",
    "    while True:\n",
    "        # Find the two closest clusters\n",
    "        cluster_to_merge = min(\n",
    "            ((i, j) for i in range(len(clusters)) for j in range(i + 1, len(clusters))),\n",
    "            key=lambda x: cluster_dist(clusters[x[0]], clusters[x[1]])\n",
    "        )\n",
    "        \n",
    "        # Calculate distance between the two closest clusters\n",
    "        min_distance = cluster_dist(clusters[cluster_to_merge[0]], clusters[cluster_to_merge[1]])\n",
    "\n",
    "        # Stop if distance is above the threshold\n",
    "        if min_distance > threshold:\n",
    "            break\n",
    "\n",
    "        # Merge the two closest clusters\n",
    "        clusters[cluster_to_merge[0]].extend(clusters[cluster_to_merge[1]])\n",
    "        clusters.pop(cluster_to_merge[1])\n",
    "        \n",
    "        merger = tuple(f\"{num + 1}.html\" for num in clusters[cluster_to_merge[0]])\n",
    "        with open(\"clusters.txt\", \"a\") as f:\n",
    "            f.write(f\"Merged clusters: {merger}\\n\")\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "66776670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete link method:\n",
      "Result: [[0, 12, 101, 152, 157, 175, 185, 487, 108, 186, 121, 290, 58, 103, 482, 1, 454, 41, 80, 164, 412, 474, 500, 84, 372, 113, 360, 275, 389, 387, 5, 6, 481, 25, 68, 133, 401, 464, 159, 354, 296, 10, 399, 455, 174, 162, 390, 273, 352, 338, 143, 254, 249, 235, 222, 180], [2, 7, 31, 34, 89, 126, 470, 495, 212, 326, 262, 260, 244, 214, 365, 43, 439, 480, 120, 194, 46, 477, 380, 111, 461, 502, 293, 243, 127, 137, 421, 136, 37, 213, 370, 300, 116, 171, 79, 98, 405, 496, 351, 94, 86], [3, 13, 99, 147, 154, 462, 445, 425, 391, 384, 167, 228, 14, 82, 437, 456, 42, 138, 176, 329, 369, 337, 288, 28, 410, 467, 285, 281, 266, 141, 237, 236, 204, 166, 184, 151], [4, 32, 129, 199, 255, 8, 26, 473, 30, 45, 458, 253, 117, 74, 179, 448, 356, 104, 78, 335, 19, 24, 438, 459, 308, 302, 297, 274, 145, 16, 69, 181, 449, 383, 301, 362, 18, 466, 40, 230, 400, 177, 494, 118, 63, 488, 378, 291, 172, 211, 271, 268, 426, 33, 71, 472, 342, 267, 339, 233, 140, 429, 146], [9, 22, 60, 398, 450, 453, 20, 130, 397, 368, 132, 95, 119, 328, 393, 436, 161, 499, 319, 304, 261, 259, 38, 66, 451, 479, 158, 51, 200, 478, 123, 413, 112, 169, 299, 135, 124, 340, 56, 91, 408, 475, 407, 265, 257, 188], [11, 64, 109, 465, 219, 422, 220, 324, 270, 128, 134, 415, 377, 435, 202, 359, 336, 331, 292, 238, 234, 231, 221], [15, 150, 170, 226, 373, 21, 29, 189, 223, 471, 307, 280, 27, 55, 469, 279, 114, 419, 277, 240, 168, 218, 52, 70, 441, 483, 388, 173, 431, 160, 428, 97, 195, 316, 153, 216, 115, 406, 276, 250, 245, 125, 205, 446, 375, 363, 191, 193, 452, 54, 67, 486, 361, 395, 394, 386, 382, 381, 182, 311, 229, 187, 349, 183, 88, 416, 295, 272, 232, 149, 90, 497], [17, 131, 190, 457, 333, 100, 81, 163, 417, 358, 287, 263, 201, 210, 207, 23, 61, 72, 76, 411, 73, 490, 53, 215, 65, 489, 49, 440, 493, 350, 294, 241, 144], [35, 50, 139, 209, 460, 36, 409, 332, 227, 367, 364, 348, 341, 320, 252, 246, 242, 239], [39, 492, 376, 347, 96, 105, 498, 312, 447, 57, 414, 442, 484, 334, 430, 501, 444, 379, 47, 87, 148, 402, 75, 330, 77, 92, 353, 321, 344, 305, 289, 286, 256, 122, 142], [44], [48, 102, 468, 392, 314, 224, 424, 284, 282, 269, 59, 423, 485, 323, 197, 366, 318, 264, 247, 93, 491, 433, 432, 385, 306, 298, 355, 198, 155], [62, 107, 403, 443, 476, 322, 315, 346, 345, 310, 85, 404, 309, 283, 208, 217], [83, 106, 463, 325, 303, 278, 396, 156, 427, 371, 258, 251, 248, 203, 206, 178], [110], [165, 420, 225, 192, 374, 357, 343, 327, 317, 196, 313, 418, 434]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Complete link method:\")\n",
    "complete_clusters = agglomerative_clustering_custom(similarity_matrix, method='complete')\n",
    "print(\"Result:\", complete_clusters)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "02f52bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken:  181.8699610233307\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time taken: \", end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
